{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# import numpy\n",
    "# import pandas\n",
    "# from pandas import DataFrame, Series\n",
    "# import argparse\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# import gc\n",
    "# #import psutil\n",
    "# # process = psutil.Process(os.getpid())\n",
    "\n",
    "# import simons_meta_data\n",
    "# import hg19_chrom_sizes\n",
    "\n",
    "# #script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "# sys.path.insert(0, '/home/kmt/simons/faststorage/people/kmt/notebooks')\n",
    "# import analysis_globals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--nr_wins', type=int, default=5, help='')\n",
    "parser.add_argument('--offset', type=int, default=100000, help='')\n",
    "parser.add_argument('--cpus', type=int, help='')\n",
    "##### Added these two options\n",
    "parser.add_argument('--min-sweep-clade-percent', dest='min_sweep_clade_percent', type=int)\n",
    "parser.add_argument('--pwdist-cutoff', dest='pwdist_cutoff', type=float)                \n",
    "# parser.add_argument('--dump-dist-twice', dest='dump_dist_twice', type=str, default=None, help='')\n",
    "parser.add_argument('dist_file_name', type=str, help='')\n",
    "parser.add_argument('sweep_data_file_name', type=str, help='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# class Args():\n",
    "#     def __init__(self):\n",
    "#         self.nr_wins = 5\n",
    "#         self.offset = 100000\n",
    "#         self.cpus = 1\n",
    "#         self.min_sweep_clade_percent = 1\n",
    "#         self.pwdist_cutoff = 5e-05\n",
    "#         self.dist_file_name = '/home/kmt/simons/faststorage/people/kmt/steps/male_dist_admix_masked_stores/male_dist_data_chrX_100kb_twice.hdf'\n",
    "#         self.sweep_data_file_name = '/home/kmt/simons/faststorage/people/kmt/steps/male_dist_admix_masked_stores/5e-05/sweep_data_5e-05_1%.hdf'\n",
    "\n",
    "# args = Args()\n",
    "\n",
    "if args.cpus:\n",
    "    nr_cpu = args.cpus\n",
    "else:\n",
    "    nr_cpu = int(os.environ.get('SLURM_JOB_CPUS_PER_NODE'))\n",
    "\n",
    "nr_wins = args.nr_wins\n",
    "offset = args.offset\n",
    "offsets = [x * offset for x in range(nr_wins)]\n",
    "window_size = len(offsets) * offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy\n",
    "import pandas\n",
    "from pandas import DataFrame, Series\n",
    "import argparse\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import gc\n",
    "\n",
    "import simons_meta_data\n",
    "import hg19_chrom_sizes\n",
    "\n",
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "sys.path.insert(0, script_dir + '/../notebooks')\n",
    "import analysis_globals\n",
    "\n",
    "\n",
    "def call_rolling_windows(df, pwdist_cutoff, min_sweep_clade_size):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # build data frame to return\n",
    "    cols = ['indiv_1', 'start', 'end', 'off']\n",
    "    result_df = pandas.DataFrame(list(df.groupby(cols).groups.keys()), columns=cols)\n",
    "\n",
    "    # Get mean dist between pairs in the 500kb window (make mean nan if any 100kb is nan):\n",
    "    win_dist = df.groupby(['indiv_1', 'indiv_2']).dist.agg(numpy.mean)\n",
    "    \n",
    "    # Build graph\n",
    "    graph = nx.Graph()\n",
    "    for tup in win_dist[win_dist <= pwdist_cutoff].reset_index().itertuples():\n",
    "        graph.add_edge(tup.indiv_1, tup.indiv_2)\n",
    "        \n",
    "    cliques = sorted(nx.algorithms.clique.find_cliques(graph), key=len)\n",
    "\n",
    "    # find swept\n",
    "    result_df['called'] = False\n",
    "    result_df['mean_clade_dist'] = numpy.nan\n",
    "    result_df['clade_size'] = numpy.nan\n",
    "    \n",
    "    for clique in cliques:\n",
    "\n",
    "        if len(clique) >= min_sweep_clade_size:\n",
    "            \n",
    "            # get indiv pairs. (in just one orientation). that is fine because df has both orientations\n",
    "\n",
    "            called = result_df.indiv_1.isin(clique)\n",
    "            #result_df['called'] = called | result_df.called # do not make it False if it is already True   \n",
    "            \n",
    "            result_df.loc[called, 'called'] = True\n",
    "            \n",
    "            # clade_size is the size of the clique\n",
    "            result_df.loc[called, 'clade_size'] = len(clique) \n",
    "            \n",
    "            # mean_clade_dist is the mean dist of the clique\n",
    "            indiv_pairs = list(itertools.combinations(sorted(clique), 2))\n",
    "            result_df.loc[called, 'mean_clade_dist'] = win_dist.loc[indiv_pairs].mean()                \n",
    "                \n",
    "    if 'dist_af' in df.columns:\n",
    "        \n",
    "        # Get mean dist between pairs in the 500kb window (make mean nan if any 100kb is nan):\n",
    "        win_dist = df.groupby(['indiv_1', 'indiv_2']).dist_af.agg(numpy.mean)\n",
    "\n",
    "        # Build graph\n",
    "        graph = nx.Graph()\n",
    "        for tup in win_dist[win_dist <= pwdist_cutoff].reset_index().itertuples():\n",
    "            graph.add_edge(tup.indiv_1, tup.indiv_2)\n",
    "\n",
    "        cliques = sorted(nx.algorithms.clique.find_cliques(graph), key=len)\n",
    "\n",
    "        # find swept\n",
    "        result_df['called_af'] = False\n",
    "        result_df['mean_clade_dist_af'] = numpy.nan\n",
    "        result_df['clade_size_af'] = numpy.nan\n",
    "\n",
    "        for clique in cliques:\n",
    "\n",
    "            if len(clique) >= min_sweep_clade_size:\n",
    "                # get indiv pairs. (in just one orientation). that is fine because df has both orientations\n",
    "\n",
    "                called = result_df.indiv_1.isin(clique)\n",
    "\n",
    "                result_df.loc[called, 'called_af'] = True\n",
    "\n",
    "                # clade_size is the size of the clique\n",
    "                result_df.loc[called, 'clade_size_af'] = len(clique) \n",
    "\n",
    "                # mean_clade_dist is the mean dist of the clique\n",
    "                indiv_pairs = list(itertools.combinations(sorted(clique), 2))\n",
    "                result_df.loc[called, 'mean_clade_dist_af'] = win_dist.loc[indiv_pairs].mean()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def call_swept(df):\n",
    "    \"\"\"\n",
    "    Takes a df with all rolling window data for an indivisual for one 100kb window.\n",
    "    Call each 100kb window as sweept if any overlapping rolling window is called as swept.\n",
    "    Compute clade size and mean clade dist as from the rolling window with the largest clade size.\n",
    "    \"\"\"\n",
    "    max_clade_size = df.clade_size.max()\n",
    "    offset_with_largest_clique = (df.groupby('off')\n",
    "                             .filter(lambda df: (df.clade_size == max_clade_size).all() and df.called.all())\n",
    "                            )\n",
    "\n",
    "    assert len(df) == len(df.off.unique()) # one row for each offset\n",
    "\n",
    "    if 'called_af' in df.columns:\n",
    "        max_clade_size_af = df.clade_size_af.max()\n",
    "        offset_with_largest_clique_af = (df.groupby('off')\n",
    "                                .filter(lambda df: (df.clade_size_af == max_clade_size_af).all() and df.called_af.all())\n",
    "                                )\n",
    "\n",
    "        return DataFrame(dict(called=[df.called.any()], \n",
    "                            clade_size=[max_clade_size],\n",
    "                            clade_mean_dist=[offset_with_largest_clique.mean_clade_dist.mean()], # actually mean over identical numbers\n",
    "                            called_af=[df.called_af.any()], \n",
    "                            clade_size_af=[max_clade_size_af],\n",
    "                            clade_mean_dist_af=[offset_with_largest_clique_af.mean_clade_dist.mean()])) # actually mean over identical numbers\n",
    "    else:\n",
    "        return DataFrame(dict(called=[df.called.any()], \n",
    "                            clade_size=[max_clade_size],\n",
    "                            clade_mean_dist=[offset_with_largest_clique.mean_clade_dist.mean()])) # actually mean over identical numbers\n",
    "    \n",
    "\n",
    "\n",
    "def _window_stats(df, pwdist_cutoff, min_sweep_clade_size):\n",
    "    if 'dist_af' in df.columns:\n",
    "        return pandas.DataFrame({\n",
    "                                'mean_dist': [df.dist.mean()],\n",
    "                                'mean_dist_to_africans': [df.loc[df.region_2 == 'Africa', 'dist'].mean()],\n",
    "                                'mean_dist_af': [df.dist_af.mean()],\n",
    "                                'mean_dist_to_africans_af': [df.loc[df.region_2 == 'Africa', 'dist_af'].mean()],\n",
    "                                'win_swept': (df.dist <= pwdist_cutoff).sum() >= \\\n",
    "                                                min_sweep_clade_size,\n",
    "                                'win_swept_af': (df.dist_af <= pwdist_cutoff).sum() >= \n",
    "                                                min_sweep_clade_size,\n",
    "                                'prop_indivs_missing': [numpy.isnan(df.dist).sum() / df.dist.size]\n",
    "                                })\n",
    "    else:\n",
    "        return pandas.DataFrame({\n",
    "                        'mean_dist': [df.dist.mean()],\n",
    "                        'win_swept': (df.dist <= pwdist_cutoff).sum() >= \\\n",
    "                                        min_sweep_clade_size,\n",
    "                        'prop_indivs_missing': [numpy.isnan(df.dist).sum() / df.dist.size]\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--nr_wins', type=int, default=5, help='')\n",
    "# parser.add_argument('--offset', type=int, default=100000, help='')\n",
    "# parser.add_argument('--cpus', type=int, help='')\n",
    "# parser.add_argument('--min-sweep-clade-percent', dest='min_sweep_clade_percent', type=int)\n",
    "# parser.add_argument('--pwdist-cutoff', dest='pwdist_cutoff', type=float)                \n",
    "# parser.add_argument('dist_file_name', type=str, help='')\n",
    "# parser.add_argument('sweep_data_file_name', type=str, help='')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# if args.cpus:\n",
    "#     nr_cpu = args.cpus\n",
    "# else:\n",
    "#     nr_cpu = int(os.environ.get('SLURM_JOB_CPUS_PER_NODE'))\n",
    "\n",
    "# nr_wins = args.nr_wins\n",
    "# offset = args.offset\n",
    "# offsets = [x * offset for x in range(nr_wins)]\n",
    "# window_size = len(offsets) * offset\n",
    "\n",
    "###################################################################\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.nr_wins = 5\n",
    "        self.offset = 100000\n",
    "        self.cpus = 1\n",
    "        self.min_sweep_clade_percent = 30\n",
    "        self.pwdist_cutoff = 5e-05\n",
    "        self.dist_file_name = '/home/kmt/simons/faststorage/people/kmt/steps/male_dist_admix_masked_stores/male_dist_data_chrX_100kb_twice.hdf'\n",
    "        self.sweep_data_file_name = '/home/kmt/simons/faststorage/people/kmt/steps/male_dist_admix_masked_stores/5e-05/sweep_data_5e-05_1%.hdf'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "if args.cpus:\n",
    "    nr_cpu = args.cpus\n",
    "else:\n",
    "    nr_cpu = int(os.environ.get('SLURM_JOB_CPUS_PER_NODE'))\n",
    "\n",
    "nr_wins = args.nr_wins\n",
    "offset = args.offset\n",
    "offsets = [x * offset for x in range(nr_wins)]\n",
    "window_size = len(offsets) * offset\n",
    "###################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_male_dist_twice = pandas.read_hdf(args.dist_file_name)\n",
    "\n",
    "# remove unused columns\n",
    "to_keep = ['indiv_1', 'indiv_2', 'start', 'end', 'dist', 'dist_af', \n",
    "           'region_1', 'region_2', 'pop_1', 'region_label_1', 'region_id_1']\n",
    "to_drop = [x for x in all_male_dist_twice.columns if x not in to_keep]\n",
    "all_male_dist_twice.drop(to_drop, axis=1, inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "nr_indiv = all_male_dist_twice.indiv_1.unique().size\n",
    "\n",
    "MIN_SWEEP_CLADE_SIZE = round(nr_indiv * args.min_sweep_clade_percent / 100)\n",
    "\n",
    "# partial version of window_stats\n",
    "import functools\n",
    "window_stats = functools.partial(_window_stats, pwdist_cutoff=args.pwdist_cutoff, min_sweep_clade_size=MIN_SWEEP_CLADE_SIZE)\n",
    "\n",
    "# merge window sweep info with distance data\n",
    "if 'dist_af' in all_male_dist_twice.columns:\n",
    "    # this is not a simulation\n",
    "    gr_cols = ['indiv_1', 'start', 'end', 'pop_1', 'region_label_1', 'region_id_1', 'region_1']\n",
    "else:\n",
    "    gr_cols = ['indiv_1', 'start', 'end']\n",
    "stats_data = (all_male_dist_twice\n",
    "        .groupby(gr_cols)\n",
    "        .apply(window_stats)\n",
    "        .reset_index(level=gr_cols)\n",
    "        )\n",
    "\n",
    "lst = list()\n",
    "# loop over five offsets of 500kb windows\n",
    "for off in offsets:\n",
    "    print(off)\n",
    "    groups = (all_male_dist_twice\n",
    "                .assign(off=off, # keep offset\n",
    "                        roll_win = lambda df: (off + df.start) // window_size) # label for rolling 500kb window\n",
    "                .groupby(['roll_win', 'off'], as_index=False)\n",
    "              \n",
    "              ### ADDED THIS TO ENSURE NO CALLS ARE MADE ON WINDOWS SMALLER THAN 500KB:\n",
    "                .filter(lambda df: df.start.unique().size == nr_wins)             \n",
    "                .groupby(['roll_win', 'off'], as_index=False)\n",
    "\n",
    "                )\n",
    "    # with Pool(nr_cpu) as p:\n",
    "    #     df = pandas.concat(p.map(call_rolling_windows, [group for name, group in groups]))\n",
    "    ##### added pwdist arg to function call\n",
    "\n",
    "    lst.append(groups.apply(call_rolling_windows, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE))\n",
    "\n",
    "del all_male_dist_twice\n",
    "gc.collect()\n",
    "\n",
    "# concatenate data frames for each offset and call windows as swept\n",
    "sweep_calls = (pandas.concat(lst)\n",
    "                .groupby(['indiv_1', 'start', 'end'])\n",
    "                .apply(call_swept)\n",
    "                .reset_index(level=['indiv_1', 'start', 'end'])\n",
    "                )\n",
    "\n",
    "del lst[:]\n",
    "gc.collect()\n",
    "\n",
    "sweep_data = stats_data.merge(sweep_calls, on=['indiv_1', 'start', 'end'])\n",
    "\n",
    "del stats_data\n",
    "del sweep_calls\n",
    "gc.collect()\n",
    "\n",
    "# get run length of swept windows and call windows as part of sweeps (ECH haplotypes)\n",
    "def run_id(sr):\n",
    "    return (sr != sr.shift()).cumsum()\n",
    "\n",
    "sweep_data['run_id'] = (sweep_data\n",
    "                        .groupby('indiv_1')['called']\n",
    "                        .apply(run_id)\n",
    "                       )\n",
    "sweep_data['run_length'] = (sweep_data\n",
    "                            .groupby(['indiv_1', 'run_id'])['run_id']\n",
    "                            .transform(numpy.size)\n",
    "                           )\n",
    "sweep_data['swept'] = numpy.bitwise_and(sweep_data['called'], \n",
    "                                        sweep_data['run_length'] >= analysis_globals.min_run_length)\n",
    "\n",
    "if 'called_af' in sweep_data.columns:\n",
    "    # this is not a simulation\n",
    "    sweep_data['run_id_af'] = (sweep_data\n",
    "                            .groupby('indiv_1')['called_af']\n",
    "                            .apply(run_id)\n",
    "                        )\n",
    "    sweep_data['run_length_af'] = (sweep_data\n",
    "                                .groupby(['indiv_1', 'run_id_af'])['run_id_af']\n",
    "                                .transform(numpy.size)\n",
    "                            )\n",
    "    sweep_data['swept_af'] = numpy.bitwise_and(sweep_data['called_af'], \n",
    "                                            sweep_data['run_length_af'] >= analysis_globals.min_run_length)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# write to hdf output file\n",
    "sweep_data.to_hdf(args.sweep_data_file_name, 'df', mode='w', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07740020751953125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dist twice 3.3942184448242188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.3942298889160156"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### no longer reading dist_file name                                    \n",
    "# male_dist_data = pandas.read_hdf(args.dist_file_name)\n",
    "\n",
    "# all_male_dist_twice = dist_twice(male_dist_data)\n",
    "###### reading twice file instead.\n",
    "\n",
    "###### NEW ##########\n",
    "# all_male_dist_twice = pandas.read_hdf(args.dist_file_name)\n",
    "all_male_dist_twice = pandas.read_hdf('test.hdf')\n",
    "\n",
    "to_keep = ['indiv_1', 'indiv_2', 'start', 'end', 'dist', 'dist_af', \n",
    "           'region_1', 'region_2', 'pop_1', 'region_label_1', 'region_id_1']\n",
    "to_drop = [x for x in all_male_dist_twice.columns if x not in to_keep]\n",
    "all_male_dist_twice.drop(to_drop, axis=1, inplace=True)\n",
    "###### NEW ##########\n",
    "\n",
    "##### defining MIN_SWEEP_CLADE_SIZE using arg to script\n",
    "nr_indiv = all_male_dist_twice.indiv_1.unique().size\n",
    "\n",
    "MIN_SWEEP_CLADE_SIZE = round(nr_indiv * args.min_sweep_clade_percent / 100)\n",
    "\n",
    "print('Loaded dist twice', process.memory_info().rss / 1024**3)\n",
    "\n",
    "process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.394367218017578, 1.2441185507923365)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.memory_info().rss / 1024**3, all_male_dist_twice.memory_usage(deep=True).sum() / 1024 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2883"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#### NEW #################\n",
    "\n",
    "## optimize data frame in script that generates dist twice frame..\n",
    "\n",
    "def optimize_data_frame(df, down_int='integer'):\n",
    "    # down_int can be 'unsigned'\n",
    "    \n",
    "    converted_df = pandas.DataFrame()\n",
    "\n",
    "    floats_optim = (df\n",
    "                    .select_dtypes(include=['float'])\n",
    "                    .apply(pandas.to_numeric,downcast='float')\n",
    "                   )\n",
    "    converted_df[floats_optim.columns] = floats_optim\n",
    "\n",
    "    ints_optim = (df\n",
    "                    .select_dtypes(include=['int'])\n",
    "                    .apply(pandas.to_numeric,downcast=down_int)\n",
    "                   )\n",
    "    converted_df[ints_optim.columns] = ints_optim\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique_values = len(df[col].unique())\n",
    "        num_total_values = len(df[col])\n",
    "        if num_unique_values / num_total_values < 0.5:\n",
    "            converted_df[col] = df[col].astype('category')\n",
    "        else:\n",
    "            converted_df[col] = df[col]\n",
    "\n",
    "    unchanged_cols = df.columns[~df.columns.isin(converted_df.columns)]\n",
    "    converted_df[unchanged_cols] = df[unchanged_cols]\n",
    "\n",
    "    # keep columns order\n",
    "    converted_df = converted_df[df.columns]      \n",
    "            \n",
    "    return converted_df\n",
    "\n",
    "#all_male_dist_twice = optimize_data_frame(all_male_dist_twice, down_int='unsigned')\n",
    "all_male_dist_twice = optimize_data_frame(all_male_dist_twice, down_int='unsigned')\n",
    "gc.collect()\n",
    "\n",
    "#### NEW #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2441185507923365, 3.394390106201172)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_male_dist_twice.memory_usage(deep=True).sum() / 1024 ** 3, process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_male_dist_twice.to_hdf('test.hdf', 'df', format='table', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_male_dist_twice = all_male_dist_twice.iloc[:100000]\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off = 0\n",
    "\n",
    "# groups = (all_male_dist_twice\n",
    "#             .assign(off=off, # keep offset\n",
    "#                     roll_win = lambda df: (off + df.start) // window_size) # label for rolling 500kb window\n",
    "#             .groupby(['indiv_1', 'roll_win', 'off'], )\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group = groups.get_group(('B_Han-3', 200, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# x = numpy.empty(5)\n",
    "# x[:] = numpy.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# x = [numpy.nan] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# Series(numpy.nan, index=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# new_df = new_call_rolling_windows(group, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# old_df = old_call_rolling_windows(group, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_df = old_call_rolling_windows(group, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE)\n",
    "# # \n",
    "# new_df.equals(old_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# off = 0\n",
    "\n",
    "# groups = (all_male_dist_twice\n",
    "#             .assign(off=off, # keep offset\n",
    "#                     roll_win = lambda df: (off + df.start) // window_size) # label for rolling 500kb window\n",
    "#             .groupby(['indiv_1', 'roll_win', 'off'], as_index=False)\n",
    "#             )\n",
    "\n",
    "# new_df = groups.apply(new_call_rolling_windows, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# off = 0\n",
    "\n",
    "# groups = (all_male_dist_twice\n",
    "#             .assign(off=off, # keep offset\n",
    "#                     roll_win = lambda df: (off + df.start) // window_size) # label for rolling 500kb window\n",
    "#             .groupby(['indiv_1', 'roll_win', 'off'])\n",
    "#             )\n",
    "\n",
    "# old_df = pandas.concat([new_call_rolling_windows(group, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE) for name, group in groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.equals(old_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #new_df.sort_values(by=['indiv_1', 'indiv_2', 'start']).head()\n",
    "# new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset: 0\n",
      "offset: 100000\n",
      "offset: 200000\n",
      "offset: 300000\n",
      "offset: 400000\n",
      "Did rolling windows 25.583786010742188\n"
     ]
    }
   ],
   "source": [
    "#### NEW #################\n",
    "\n",
    "lst = list()\n",
    "# loop over five offsets of 500kb windows\n",
    "for off in offsets:\n",
    "    groups = (all_male_dist_twice\n",
    "                .assign(off=off, # keep offset\n",
    "                        roll_win = lambda df: (off + df.start) // window_size) # label for rolling 500kb window\n",
    "#                 .groupby(['indiv_1', 'roll_win', 'off'])\n",
    "                .groupby(['indiv_1', 'roll_win', 'off'], as_index=False)\n",
    "                )\n",
    "    # with Pool(nr_cpu) as p:\n",
    "    #     df = pandas.concat(p.map(call_rolling_windows, [group for name, group in groups]))\n",
    "    ##### added pwdist arg to function call\n",
    "                \n",
    "#     df = pandas.concat([call_rolling_windows(group, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE) for name, group in groups])\n",
    "#     lst.append(df)\n",
    "\n",
    "    lst.append(groups.apply(call_rolling_windows, args.pwdist_cutoff, MIN_SWEEP_CLADE_SIZE))\n",
    "\n",
    "\n",
    "#### NEW #################\n",
    "    \n",
    "    \n",
    "    print('offset:', off)\n",
    "    \n",
    "print('Did rolling windows', process.memory_info().rss / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.58379364013672"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pandas.concat(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_hdf('df.hdf', 'df', format='table', mode='w')\n",
    "# process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.memory_usage(deep=True).sum() for x in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del lst[:]\n",
    "# gc.collect()\n",
    "# process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pandas.read_hdf('df.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups = df.groupby(['indiv_1', 'start'])\n",
    "\n",
    "# group = groups.get_group(('B_Han-3', 50000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups.ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# call_swept(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sweep_calls = (pandas.concat(lst)\n",
    "#                .groupby(['indiv_1', 'start'])\n",
    "#                 .apply(call_swept)\n",
    "#                 .reset_index(level=['indiv_1', 'start'])\n",
    "#                 )\n",
    "\n",
    "# process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_calls.to_hdf('tmp.hdf', 'df', format='table', mode='w')\n",
    "# process.memory_info().rss / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 25.510910034179688\n",
      "after 9.854888916015625\n",
      "Did sweep calls\n"
     ]
    }
   ],
   "source": [
    "# concatenate data frames for each offset and call windows as swept\n",
    "sweep_calls = (pandas.concat(lst)\n",
    "                .groupby(['indiv_1', 'start'])\n",
    "                .apply(call_swept)\n",
    "                .reset_index(level=['indiv_1', 'start'])\n",
    "                )\n",
    "\n",
    "print('before', process.memory_info().rss / 1024**3)\n",
    "#### NEW #################\n",
    "del lst[:]\n",
    "gc.collect()\n",
    "#### NEW #################\n",
    "print('after', process.memory_info().rss / 1024**3)\n",
    "\n",
    "print('Did sweep calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 9.861331939697266\n",
      "after 8.914207458496094\n",
      "Did window stats\n",
      "CPU times: user 11min 57s, sys: 10.2 s, total: 12min 7s\n",
      "Wall time: 12min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##### partial version of window_stats\n",
    "import functools\n",
    "window_stats = functools.partial(_window_stats, pwdist_cutoff=args.pwdist_cutoff, min_sweep_clade_size=MIN_SWEEP_CLADE_SIZE)\n",
    "\n",
    "# merge window sweep info with distance data\n",
    "if 'dist_af' in all_male_dist_twice.columns:\n",
    "    # this is not a simulation\n",
    "    gr_cols = ['indiv_1', 'start', 'end', 'pop_1', 'region_label_1', 'region_id_1', 'region_1']\n",
    "else:\n",
    "    gr_cols = ['indiv_1', 'start', 'end']\n",
    "df = (all_male_dist_twice\n",
    "        .groupby(gr_cols)\n",
    "        .apply(window_stats)\n",
    "        .reset_index(level=gr_cols)\n",
    "        )\n",
    "sweep_data = df.merge(sweep_calls, on=['indiv_1', 'start'])\n",
    "\n",
    "print('before', process.memory_info().rss / 1024**3)\n",
    "#### NEW #################\n",
    "del df\n",
    "del all_male_dist_twice\n",
    "gc.collect()\n",
    "#### NEW #################\n",
    "print('after', process.memory_info().rss / 1024**3)\n",
    "print('Did window stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_data.to_hdf('tmp2.hdf', 'df', format='table', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 8.807376861572266\n",
      "after 8.807132720947266\n",
      "Did run_length and labeling of swept\n",
      "CPU times: user 6.22 s, sys: 51.6 ms, total: 6.27 s\n",
      "Wall time: 6.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get run length of swept windows and call windows as part of sweeps (ECH haplotypes)\n",
    "def run_id(sr):\n",
    "    return (sr != sr.shift()).cumsum()\n",
    "\n",
    "sweep_data['run_id'] = (sweep_data\n",
    "                        .groupby('indiv_1')['called']\n",
    "                        .apply(run_id)\n",
    "                       )\n",
    "sweep_data['run_length'] = (sweep_data\n",
    "                            .groupby(['indiv_1', 'run_id'])['run_id']\n",
    "                            .transform(numpy.size)\n",
    "                           )\n",
    "sweep_data['swept'] = numpy.bitwise_and(sweep_data['called'], \n",
    "                                        sweep_data['run_length'] >= analysis_globals.min_run_length)\n",
    "\n",
    "if 'called_af' in sweep_data.columns:\n",
    "    # this is not a simulation\n",
    "    sweep_data['run_id_af'] = (sweep_data\n",
    "                            .groupby('indiv_1')['called_af']\n",
    "                            .apply(run_id)\n",
    "                        )\n",
    "    sweep_data['run_length_af'] = (sweep_data\n",
    "                                .groupby(['indiv_1', 'run_id_af'])['run_id_af']\n",
    "                                .transform(numpy.size)\n",
    "                            )\n",
    "    sweep_data['swept_af'] = numpy.bitwise_and(sweep_data['called_af'], \n",
    "                                            sweep_data['run_length_af'] >= analysis_globals.min_run_length)\n",
    "\n",
    "print('before', process.memory_info().rss / 1024**3)\n",
    "gc.collect()\n",
    "print('after', process.memory_info().rss / 1024**3)\n",
    "\n",
    "print('Did run_length and labeling of swept')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_data.to_hdf('tmp3.hdf', 'df', format='table', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 8.806892395019531\n",
      "after 8.806892395019531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# if args.dump_dist_twice:\n",
    "#     all_male_dist_twice.to_hdf(args.dump_dist_twice, 'df', \n",
    "#                                data_columns=['start', 'end', \n",
    "#                                          'indiv_1', 'indiv_2', \n",
    "#                                          'pop_1', 'pop_2', \n",
    "#                                          'region_label_1', 'region_label_2'],\n",
    "#                                format='table', mode='w')\n",
    "\n",
    "    \n",
    "# write to hdf output file\n",
    "print('before', process.memory_info().rss / 1024**3)\n",
    "sweep_data.to_hdf('out.hdf', 'df', format='table', mode='w')\n",
    "print('after', process.memory_info().rss / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
